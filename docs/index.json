[{"content":"There has been a large shift in the industry in recent years to moving to python for data engineering. Spark one of the leading distributed data processing engines has great support for python via the Pyspark library, allowing users to write high performance distributed data processing on spark from the comfort of python.\nIn this article I want to touch on some high-level topics relating to using pyspark which I have seen skimmed over at companies where I have worked at leading to varied and generally poor data pipeline quality.\nTo python or not to python.. One of point important points to consider when building a pipeline is whether python is the best language to use for that use-case? Many pipelines can be more concisely written in SQL and more performant in Java/Scala.\nGenerally I believe the best approach is to use the simplest solution which meets the requirements, and then only if necessary move to a more complex solution. No one is going to be impressed if you write a spark job in Java when it could have been done in SQL if both could meet the requirements.\nAdditionally if we consider Databricks, if you have a lot of jobs you can end up creating a lot of job clusters and optimizing the sizing can be difficult. Using SQL allows you to target a sql warehouse cluster and let databricks scale this, which could end up minimising latency and reducing costs.\nGenerally the flow I have in mind is as below:\ngraph TD; A[SQL] --more flexibility--\u003e B[Python] --more performance--\u003e C[Java/Scala] There is a however a trade-off for the additional flexibility and performance and that is technical debt, and we should choose the best tool for the users and capabilities of the team. A tech company will have no problem building and running scala jobs, but a company with a lot of data analysts would be best suited to stick to sql with the odd python job.\nNotebooks != Production To me notebooks have been marketed as the \u0026ldquo;low code\u0026rdquo; approach for data engineering, but in reality they are a double edged sword. They are great for exploration and prototyping, but when it comes to production they are a nightmare.\nCompanies like Databricks heavily promote using notebooks, adding features like version control, co-pilots, etc. Their solutions architects sales people will even recommend them throughout the layers of the company as best practice. There is so much marketing from these companies that some colleagues didn\u0026rsquo;t even know you could submit jobs in another way. Remember if you drink all the cool-aid you aren\u0026rsquo;t using spark you are using databricks.\nTestability is the first big issue which comes to mind with notebooks, If we consider the databricks variety they have a lot of magic variables injected such as the spark context, dbutils, widgets, etc. Additionally if you want to write unit tests, these should be run in a CI/CD pipeline, and running a notebook in a cicd pipeline sounds like a nightmare. So you would have to convert the notebook to a python package, which then begs the question why do you need the notebook anymore?\nThere are other issues such as development in a proper IDE being much more efficient than in a notebook, that it\u0026rsquo;s easier to reuse code between python packages.\nWhat then is the solution? I would recommend using notebooks for exploration and prototyping if you need to, but then taking the explicit step of converting the code to a python package for production. This way you get the best of both worlds, and you can ensure that your code is testable, maintainable and reusable.\nTo write performant code, use the pyspark api and connectors you must Did you know all the non-pyspark python code you run only executes on the driver node? Yea that 5 node spark cluster you have is only running with maximum 20% utilization, sounds expensive and slow right?\nYou would be surprised in how many instances I have seen write code like below, which fetches synchronously pulls data from an api and then creates a dataframe from it. Only the last 4 lines are executed in a distributed fashion. The writing is done using a connector which is optimised for distributed writing.\nimport pyspark.sql.functions as f import requests # synchronous fetch on driver node record_list = [] records = requests.get(\u0026#34;https://api.example.com/records\u0026#34;).json() for record in records: record_list.append(record) # create dataframe from list (backed by RDDs which are distributed) record_df = spark.createDataFrame(record_list) # distributed execution bad_records = record_df.filter(f.col(\u0026#34;status\u0026#34;) == \u0026#34;bad\u0026#34;)) good_records = record_df.filter(f.col(\u0026#34;status\u0026#34;) == \u0026#34;good\u0026#34;)) good_records.write.table(\u0026#34;product_name.staging.good_records\u0026#34;) bad_records.write.table(\u0026#34;product_name.staging.bad_records\u0026#34;) One of the most important things to remember when working with spark via python - is to heavily use built in functions from pyspark - and I would be more strict use only the dataframe api, with UDFs only if necessary. These functions utilise the native and high performance jvm (or c++*) libraries of spark to ensure your data is processed blazingly fast in a distributed fashion.\nI have found the dataframe api to be brilliant for most use-cases, have supported a lot of colleagues over the years convert their python code into the dataframe api with huge performance improvements. I would recommend avoiding the RDD api unless you need to, as it is much easier to write non-performant code.\nI would also encourage you to use native spark connectors for reading and writing data, as these are written in scala and optimised for distributed reading and writing. However Spark 4.0 will expose a python data source API to make building distributed connectors in python possible. As the spark kafka connector is very good, I would recommend using kafka as the common interface for streaming data, replicating data from other sources using tools like kafka connect. Batch data can be easily and efficiently read from s3 or other storage systems using the native spark connectors.\n| Note: you can use pandas api on spark but honestly don\u0026rsquo;t it was essentially integrated because there is a lot of legacy pandas code, modern libraries like polars have a more consistent api and are similar to the dataframe api in that both are based on lazy execution.\nChaining transformations One thing I see quite often is a pipeline which started simple, but as requirements grew the pipeline became more complex and the logic becomes harder to maintain. Here the transform function can be used to chain complex transformations together, and keep the codebase clean and maintainable by breaking the complexity into logically function chunks.\nBelow is an example from a pokemon interview challenge I did a while back, where the code is broken into smaller functions which are chained together to create the final dataframe, imagine what this would look like if it all was one big chain - yuck! I hope you can see how this can be used as a tool to keep the codebase clean and maintainable, you could imagine that each of these functions was a requirement and so I could unit test each of them to ensure I am meeting the requirements.\ndef load_pokemon(spark: SparkSession, pokemon_path: str) -\u0026gt; DataFrame: return spark.read \\ .format(\u0026#34;json\u0026#34;) \\ .option(\u0026#34;multiline\u0026#34;, \u0026#34;true\u0026#34;) \\ .schema(detailed_pokemon_schema) \\ .load(pokemon_path) def parse_games(df: DataFrame, games=[\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;leafgreen\u0026#34;, \u0026#34;white\u0026#34;]): \u0026#34;\u0026#34;\u0026#34;filter for only pokemon that are specified in the game list\u0026#34;\u0026#34;\u0026#34; group_cols = [\u0026#34;id\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;base_experience\u0026#34;, \u0026#34;weight\u0026#34;, \u0026#34;height\u0026#34;, \u0026#34;order\u0026#34;, \u0026#34;types\u0026#34;, \u0026#34;sprites\u0026#34;] return df \\ .withColumn(\u0026#34;games\u0026#34;, f.explode(\u0026#34;game_indices.version.name\u0026#34;)) \\ .filter(f.col(\u0026#34;games\u0026#34;).isin(games)) \\ .groupBy(*group_cols) \\ .agg(f.collect_list(\u0026#34;games\u0026#34;).alias(\u0026#34;games\u0026#34;)) def parse_types(df: DataFrame): \u0026#34;\u0026#34;\u0026#34;parse types into a mapping\u0026#34;\u0026#34;\u0026#34; df_join = df \\ .withColumn(\u0026#34;types\u0026#34;, f.explode(\u0026#34;types\u0026#34;)) \\ .withColumn(\u0026#34;type_slot\u0026#34;, f.concat(f.lit(\u0026#34;type_slot_\u0026#34;), f.col(\u0026#34;types.slot\u0026#34;))) \\ .withColumn(\u0026#34;type_name\u0026#34;, f.col(\u0026#34;types.type.name\u0026#34;)) \\ .groupBy(\u0026#34;name\u0026#34;).pivot(\u0026#34;type_slot\u0026#34;).agg(f.first(\u0026#34;type_name\u0026#34;)) return df.join(df_join, on=\u0026#34;name\u0026#34;).drop(\u0026#34;types\u0026#34;) def parse_sprite(df: DataFrame): \u0026#34;\u0026#34;\u0026#34;parse front_default sprite to be used a sprite\u0026#34;\u0026#34;\u0026#34; return df \\ .withColumn(\u0026#34;sprite\u0026#34;, f.col(\u0026#34;sprites.front_default\u0026#34;)) \\ .drop(\u0026#34;sprites\u0026#34;) def calculate_bmi(df: DataFrame): \u0026#34;\u0026#34;\u0026#34;calculates pokemon bmi (weight / height^2)\u0026#34;\u0026#34;\u0026#34; return df.withColumn(\u0026#34;bmi\u0026#34;, f.col(\u0026#34;weight\u0026#34;) / f.pow(f.col(\u0026#34;height\u0026#34;), f.lit(2))) def capitalize_name(df: DataFrame): \u0026#34;\u0026#34;\u0026#34;capitalizes first letter of pokemon name\u0026#34;\u0026#34;\u0026#34; capitalize_first = f.udf(lambda x: str.capitalize(x)) return df.withColumn(\u0026#34;name\u0026#34;, capitalize_first(\u0026#34;name\u0026#34;)) if __name__ == \u0026#34;__main__\u0026#34;: \u0026#34;\u0026#34;\u0026#34;sample code\u0026#34;\u0026#34;\u0026#34; ss = SparkSession.builder.master(\u0026#34;local[*]\u0026#34;).appName(\u0026#34;pokemon\u0026#34;).getOrCreate() df_poke = load_pokemon(ss, \u0026#34;data/raw/pokemon/*.json\u0026#34;) \\ .withColumn(\u0026#34;games\u0026#34;, f.explode(\u0026#34;game_indices.version.name\u0026#34;)) \\ .filter(f.col(\u0026#34;games\u0026#34;).isin(games)) \\ .groupBy(*group_cols) \\ .agg(f.collect_list(\u0026#34;games\u0026#34;).alias(\u0026#34;games\u0026#34;)) .transform(parse_sprite) \\ .transform(parse_types) \\ .transform(calculate_bmi) \\ .transform(capitalize_name) df_poke.write.mode(\u0026#34;overwrite\u0026#34;).parquet(\u0026#34;pokemon_processed\u0026#34;) Summary I hope this somewhat high-level collection of thoughts can help when building spark pipelines in python. I have seen a lot of pipelines which have been built in a sub-optimal way, and I hope this can help you avoid some of the pitfalls I have seen. If I write a part 2 it will be around testing and deployment side of this.\nI\u0026rsquo;m not using spark that much at my current company as we are dealing with more complex real-time streaming use-cases, where we are using DataFlow (Apache Beam) together with BigQuery, Kafka, Airflow \u0026amp; DBT.\nLet me know if you have any thoughts or questions, I would love to hear them.\n","permalink":"https://kirksw.github.io/posts/pyspark-part1/","summary":"There has been a large shift in the industry in recent years to moving to python for data engineering. Spark one of the leading distributed data processing engines has great support for python via the Pyspark library, allowing users to write high performance distributed data processing on spark from the comfort of python.\nIn this article I want to touch on some high-level topics relating to using pyspark which I have seen skimmed over at companies where I have worked at leading to varied and generally poor data pipeline quality.","title":"Starting with (py)Spark"},{"content":"Over the past few years I have worked at different companies building data lakehouses/meshes, and one challenge that has been constant is the need to understand the state of things (cloud, platform, pipeline, computers, etc) and how to document and mutate the state of these in a straightforward manner.\nWhy declarative systems? Lets say you were tasked with ensuring that all cloud storage is configured in a manner which matches the new standards put in place by the security team.\nPerhaps you would start by documenting the current state of things, probably by polling this data from an api, then you would need to figure out what needs to be changed, and then finally how to change the state of things to match the new standards.\nHowever imagine if the state of these buckets was defined declaratively as code, you then could easily see the state as defined in the code, and then make changes to the code to match the new standards.\nTooling like terraform, kubernetes, an nix are all examples of industry leading tooling that allow you to define the state of things in code.\nIntroduction to terraform The big aha moment for me was when I started using terraform to manage Azure cloud infrastructure. Terraform uses a language called HCL (HashiCorp Configuration Language) to define the state, a simple example of this is shown below.\n\u0026#34;resource\u0026#34; \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example-resources\u0026#34; location = \u0026#34;West Europe\u0026#34; } \u0026#34;resource\u0026#34; \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;examplestorageaccount\u0026#34; resource_group_name = azurerm_resource_group.example.name location = azurerm_resource_group.example.location account_tier = \u0026#34;Standard\u0026#34; account_replication_type = \u0026#34;LRS\u0026#34; } You can then run terraform plan to see what changes will be made to the state, and then terraform apply to make the changes if you are happy with the plan. No where in this code do we define how to make the changes, terraform figures this out for us, all thanks to the plethora of providers that are available.\nNow with infrastructure to check the state I can grep the codebase, and making a change is a simple as a pull request. Of course there are some challenges, as drift can occur due to external changes, but these can be mitigated with good practices.\nDown the nix rabbit hole I have done a lot of distro hopping over the years, and could never really find one that allowed me to have the level of control I wanted over my system - letting me use bleeding edge software, but also ensuring that everything was stable. My arch system would always break due to some update to nvidia drivers, and my ubuntu system would always be out of date.\nAdditionally to ensure that my development environment was consistent I made a dotfiles repo, which I would use to install all the software I needed on a new system. However this was always a pain as I used mac/wsl for work, and linux for personal use and had to make this work for all the systems.\nA then colleague of mine introduced me to nix, and I was blown away by the power of the system and was surprised it wasn\u0026rsquo;t more popular. Nix uses a functional programming language to define the state of the system, and then uses the nix package manager to ensure that the system is in the desired state - this should be starting to sound familiar.\nI\u0026rsquo;m not going to lie - it was a steep learning curve and the ecosystem is quite fragmented. There are nix flakes which introduces dependency locking, however it is still considered experimental even though most people are using them. Also there are partnering modules such as darwin-nix which manages mac clients and home-manage which manages setting up home environments (ala dotfiles). You also need to choose a release channel or choose unstable, I typically use the latest stable release but use overlays to get the latest software where required.\nNix has finally enabled me to have a consistent and declarative development environment across all my systems (controlling software and configurations), and I can now easily install software on a new system by running a couple of commands. Day one is now a breeze I am up and running in minutes.\nKubernetes Kubernetes is another example of a system which allows you to define the state of the system in code. You define the desired state in a yaml file called a manifest and then apply this to the cluster. Kubernetes will then figure out how to make the changes to the cluster to match the desired state. This like terraform allows you easily inspect the diff and know what changes will be made.\nI won\u0026rsquo;t lie, kubernetes is a beast and there is a lot to learn, but the power of the system is undeniable. There are tools like helm and kustomize which allow templating/overlay type functionality the yaml manifests (which terraform and nix have built in because they are programming languages). Also there are tools like argo-cd which allow you to manage the state of the cluster in a gitops manner, meaning that it will constantly poll the git repo and ensure that the cluster is in the desired state (something that needs to be handled manually with terraform).\nSummary I hope this post has given you some insight into the power of declaratively defining systems, and how they can be used to manage the state of things in a straightforward manner. I have only scratched the surface of the tools available, and I am excited to see what the future holds for this space.\nAs with anything there is a trade-off in that more work needs to be put in upfront. Much like writing tests or cicd pipelines, the benefits are not immediately apparent, but over time the investment will pay off and you will be thanking yourself if you ever need to rebuild the system from scratch.\nPlease let me know if you would like me to expand on any of the topics mentioned in this post.\nReferences Terraform terraform azure provider aws provider gcp provider\nNix my nix config nixos nix flakes nix-darwin home-manager\nKubernetes kubernetes helm kustomize argo-cd\n","permalink":"https://kirksw.github.io/posts/road-to-declarative/","summary":"Over the past few years I have worked at different companies building data lakehouses/meshes, and one challenge that has been constant is the need to understand the state of things (cloud, platform, pipeline, computers, etc) and how to document and mutate the state of these in a straightforward manner.\nWhy declarative systems? Lets say you were tasked with ensuring that all cloud storage is configured in a manner which matches the new standards put in place by the security team.","title":"Road to Declarative Systems"},{"content":"It\u0026rsquo;s not suprise that new developers are confused when it comes to developing python libraries, there has been major changes over the years when it comes to how to work with python packages, and currently you may ask yourself what to use out of setup.py, setup.cfg and pyproject.toml. Before we look at these, it is important to first make sure we properly understand python modules and packages.\nPython Modules In python a module is simply a file with a .py extension which contains related code, which could be functions, classes, variables or etc.\nFor example lets define a function to greet a user:\ndef hello_name(name): print(f\u0026#34;hello {name}\u0026#34;) To have this function store in a welcome module, we just need to save this code in a file named welcome.py.\nThis module can be used in one of two ways, either by importing the entire module, or importing a specific function. Never use from module import * as then you have no transparency on where certain functions/classes were imported from.\nimport welcome welcome.hello_name(\u0026#34;bob\u0026#34;) # prints \u0026#34;hello bob\u0026#34; from welcome import hello_name hello_name(\u0026#34;tim\u0026#34;) # prints \u0026#34;hello tim\u0026#34; Module introduce several benefits:\nImproved development Code re-uses Separate namespaces Finally a note on how python finds these modules. When import welcome is evaluated, the interpreter will search three locations.\nThe directory containing the input script (or current directly when no file is specified) PYTHONPATH (a list of directories with the same syntax as PATH) The installation-dependant default (by convention including a site-packages directory handled by the site module). Python Packages Python collection of modules intended to be installed and used together. When developing a large application you will probably end up with multiple modules which need to be organized, this is what a package does.\nIn python a package (or subpackage) is a folder containing one or more modules and a __init__.py file, which can be empty or contain some initialization code for the package.\nWe can import certain modules from any of these packages using dot notation. For example to import module_1 from the above package, we could use either of the following code snippets\nimport dairy_processing.calculations.density or\nfrom dairy_processing.calculations import density Note: We can also import specific functions as shown in the python modules section.\nPackaging flow Publishing a package requires a flow from the author’s source code to an end user’s Python environment. The steps to achieve this are:\nHave a source tree containing the package. This is typically a checkout from a version control system (VCS).\nPrepare a configuration file describing the package metadata (name, version and so forth) and how to create the build artifacts. For most packages, this will be a pyproject.toml file, maintained manually in the source tree.\nCreate build artifacts to be sent to the package distribution service (usually PyPI); these will normally be a source distribution (“sdist”) and one or more built distributions (“wheels”). These are made by a build tool using the configuration file from the previous step. Often there is just one generic wheel for a pure Python package.\nUpload the build artifacts to the package distribution service.\nAt that point, the package is present on the package distribution service. To use the package, end users must:\nDownload one of the package’s build artifacts from the package distribution service.\nInstall it in their Python environment, usually in its site-packages directory. This step may involve a build/compile step which, if needed, must be described by the package metadata.\nThese last 2 steps are typically performed by pip when an end user runs pip install.\nPyenv and Poetry to the rescue Poetry is a package which helps manage python packaging and dependency management. Pyenv allows you to manage multiple python versions on your computer. Installation of these tools is out of the scope of this guide.\nFirst we want to ensure our package is using the correct python version, this can be done by running pyenv local 3.10.5 from within the root directory (note: multiple versions can be selected using pyenv local 3.8.13 3.9.13 3.10.5 which would be required to run tox locally). Next we want to init a poetry project, which can be done using poetry init. Don\u0026rsquo;t worry about specifiying dependencies interactively as these can be added easily later, it\u0026rsquo;s important that the name is same as the name of the root package.\nIf you check the pyproject.toml file you should have something similar to below.\n[tool.poetry] name = \u0026#34;rootpackage\u0026#34; version = \u0026#34;0.0.0\u0026#34; description = \u0026#34;test package\u0026#34; authors = [\u0026#34;Your Name (your.name@email.com)\u0026#34;] [tool.poetry.dependencies] python = \u0026#34;^3.10\u0026#34; [tool.poetry.dev-dependencies] [build-system] requires = [\u0026#34;poetry-core\u0026gt;=1.0.0\u0026#34;] build-backend = \u0026#34;poetry.core.masonry.api\u0026#34; You need to ensure that your existing package is in a subfolder ./rootpackage or ./src/rootpackage and that there is a __init__.py file present. If your package needs dependencies, these can be added with poetry add {depa} {depb}, development dependencies can be added using the -D flag poetry add -D pytest.\nYou can install the package and dependencies locally using poetry install, you can build the package using poetry build and you can publish the package using poetry publish.\nLibrary inconsistency.. In theory we can use pyproject.toml to define all our project metadata and configuration for different tooling, however reality is a little more complicated. Some tools fully support pyproject.toml, some have partial support and some don\u0026rsquo;t support it at all. In practice this means you may have some files like tox.ini and setup.cfg to contain certain configurations.\nflake8 Flake8 is one the current python packages which chooses to ignore the pyproject.toml file, meaning that the easiest way to setup the configuration is to add code snippet to one of either setup.cfg, tox.ini or .flake8\n[flake8] max-line-length = 88 extend-ignore = E203 per-file-ignores = __init__.py:F401 pytest pytest is a library which has implemented a bridge to the existing .ini configuration from version 6.0, in future they will fully utilize the rich TOML format and as such have reserved [tool.pytest] for this future use.\n# pyproject.toml [tool.pytest.ini_options] minversion = \u0026#34;6.0\u0026#34; addopts = \u0026#34;-ra -q\u0026#34; testpaths = [ \u0026#34;tests\u0026#34;, \u0026#34;integration\u0026#34;, ] tox Tox has partial support for pyproject.toml by offering to inline existing ini-style fomat under the tool.tox.legacy_tox_ini key as a multi-line string. An example of this is shown below.\n[tool.tox] legacy_tox_ini = \u0026#34;\u0026#34;\u0026#34; [tox] envlist = py27,py36 [testenv] deps = pytest \u0026gt;= 3.0.0, \u0026lt;4 commands = pytest \u0026#34;\u0026#34;\u0026#34; Typically it is easier to avoid this and use the tox.ini, tox are targetting full toml support with their new release.\nSummary Python has moved away from setup.py and setup.cfg to the new pyproject.toml file, most tools support pyproject.toml in some form, however don\u0026rsquo;t expect them all to be perfect, and there are some who just don\u0026rsquo;t support it yet. It is now easier than ever to setup a new package, but there is a lot of conflicting documentation due to this migration.\nPoetry is a powerful tool to assist with managing packaging, and we didn\u0026rsquo;t even scratch the surface here. In a future post I will walk through how this is actually setup and used in a DevOps pipeline along with all the common tooling (pypi, flake8, sphinx, coverage, pytest). There are also alternatives like hatch worth investigation, and also now more than ever, it is worth considering whether these tools are even required.\n","permalink":"https://kirksw.github.io/posts/python-packaging-part1/","summary":"It\u0026rsquo;s not suprise that new developers are confused when it comes to developing python libraries, there has been major changes over the years when it comes to how to work with python packages, and currently you may ask yourself what to use out of setup.py, setup.cfg and pyproject.toml. Before we look at these, it is important to first make sure we properly understand python modules and packages.\nPython Modules In python a module is simply a file with a .","title":"Python Packaging with Poetry"}]
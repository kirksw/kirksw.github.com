[{"content":"Over the past few years I have worked at different companies building data lakehouses/meshes, and one challenge that has been constant is the need to understand the state of things (cloud, platform, pipeline, computers, etc) and how to document and mutate the state of these in a straightforward manner.\nWhy declarative systems? Lets say you were tasked with ensuring that all cloud storage is configured in a manner which matches the new standards put in place by the security team.\nPerhaps you would start by documenting the current state of things, probably by polling this data from an api, then you would need to figure out what needs to be changed, and then finally how to change the state of things to match the new standards.\nHowever imagine if the state of these buckets was defined declaratively as code, you then could easily see the state as defined in the code, and then make changes to the code to match the new standards.\nTooling like terraform, kubernetes, an nix are all examples of industry leading tooling that allow you to define the state of things in code.\nIntroduction to terraform The big aha moment for me was when I started using terraform to manage Azure cloud infrastructure. Terraform uses a language called HCL (HashiCorp Configuration Language) to define the state, a simple example of this is shown below.\n\u0026#34;resource\u0026#34; \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example-resources\u0026#34; location = \u0026#34;West Europe\u0026#34; } \u0026#34;resource\u0026#34; \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;examplestorageaccount\u0026#34; resource_group_name = azurerm_resource_group.example.name location = azurerm_resource_group.example.location account_tier = \u0026#34;Standard\u0026#34; account_replication_type = \u0026#34;LRS\u0026#34; } You can then run terraform plan to see what changes will be made to the state, and then terraform apply to make the changes if you are happy with the plan. No where in this code do we define how to make the changes, terraform figures this out for us, all thanks to the plethora of providers that are available.\nNow with infrastructure to check the state I can grep the codebase, and making a change is a simple as a pull request. Of course there are some challenges, as drift can occur due to external changes, but these can be mitigated with good practices.\nDown the nix rabbit hole I have done a lot of distro hopping over the years, and could never really find one that allowed me to have the level of control I wanted over my system - letting me use bleeding edge software, but also ensuring that everything was stable. My arch system would always break due to some update to nvidia drivers, and my ubuntu system would always be out of date.\nAdditionally to ensure that my development environment was consistent I made a dotfiles repo, which I would use to install all the software I needed on a new system. However this was always a pain as I used mac/wsl for work, and linux for personal use and had to make this work for all the systems.\nA then colleague of mine introduced me to nix, and I was blown away by the power of the system and was surprised it wasn\u0026rsquo;t more popular. Nix uses a functional programming language to define the state of the system, and then uses the nix package manager to ensure that the system is in the desired state - this should be starting to sound familiar.\nI\u0026rsquo;m not going to lie - it was a steep learning curve and the ecosystem is quite fragmented. There are nix flakes which introduces dependency locking, however it is still considered experimental even though most people are using them. Also there are partnering modules such as darwin-nix which manages mac clients and home-manage which manages setting up home environments (ala dotfiles). You also need to choose a release channel or choose unstable, I typically use the latest stable release but use overlays to get the latest software where required.\nNix has finally enabled me to have a consistent and declarative development environment across all my systems (controlling software and configurations), and I can now easily install software on a new system by running a couple of commands. Day one is now a breeze I am up and running in minutes.\nKubernetes Kubernetes is another example of a system which allows you to define the state of the system in code. You define the desired state in a yaml file called a manifest and then apply this to the cluster. Kubernetes will then figure out how to make the changes to the cluster to match the desired state. This like terraform allows you easily inspect the diff and know what changes will be made.\nI won\u0026rsquo;t lie, kubernetes is a beast and there is a lot to learn, but the power of the system is undeniable. There are tools like helm and kustomize which allow templating/overlay type functionality the yaml manifests (which terraform and nix have built in because they are programming languages). Also there are tools like argo-cd which allow you to manage the state of the cluster in a gitops manner, meaning that it will constantly poll the git repo and ensure that the cluster is in the desired state (something that needs to be handled manually with terraform).\nSummary I hope this post has given you some insight into the power of declaratively defining systems, and how they can be used to manage the state of things in a straightforward manner. I have only scratched the surface of the tools available, and I am excited to see what the future holds for this space.\nAs with anything there is a trade-off in that more work needs to be put in upfront. Much like writing tests or cicd pipelines, the benefits are not immediately apparent, but over time the investment will pay off and you will be thanking yourself if you ever need to rebuild the system from scratch.\nPlease let me know if you would like me to expand on any of the topics mentioned in this post.\nReferences Terraform terraform azure provider aws provider gcp provider\nNix my nix config nixos nix flakes nix-darwin home-manager\nKubernetes kubernetes helm kustomize argo-cd\n","permalink":"http://localhost:1313/posts/road-to-declarative/","summary":"Over the past few years I have worked at different companies building data lakehouses/meshes, and one challenge that has been constant is the need to understand the state of things (cloud, platform, pipeline, computers, etc) and how to document and mutate the state of these in a straightforward manner.\nWhy declarative systems? Lets say you were tasked with ensuring that all cloud storage is configured in a manner which matches the new standards put in place by the security team.","title":"Road to Declarative Systems"},{"content":"It\u0026rsquo;s not suprise that new developers are confused when it comes to developing python libraries, there has been major changes over the years when it comes to how to work with python packages, and currently you may ask yourself what to use out of setup.py, setup.cfg and pyproject.toml. Before we look at these, it is important to first make sure we properly understand python modules and packages.\nPython Modules In python a module is simply a file with a .py extension which contains related code, which could be functions, classes, variables or etc.\nFor example lets define a function to greet a user:\ndef hello_name(name): print(f\u0026#34;hello {name}\u0026#34;) To have this function store in a welcome module, we just need to save this code in a file named welcome.py.\nThis module can be used in one of two ways, either by importing the entire module, or importing a specific function. Never use from module import * as then you have no transparency on where certain functions/classes were imported from.\nimport welcome welcome.hello_name(\u0026#34;bob\u0026#34;) # prints \u0026#34;hello bob\u0026#34; from welcome import hello_name hello_name(\u0026#34;tim\u0026#34;) # prints \u0026#34;hello tim\u0026#34; Module introduce several benefits:\nImproved development Code re-uses Separate namespaces Finally a note on how python finds these modules. When import welcome is evaluated, the interpreter will search three locations.\nThe directory containing the input script (or current directly when no file is specified) PYTHONPATH (a list of directories with the same syntax as PATH) The installation-dependant default (by convention including a site-packages directory handled by the site module). Python Packages Python collection of modules intended to be installed and used together. When developing a large application you will probably end up with multiple modules which need to be organized, this is what a package does.\nIn python a package (or subpackage) is a folder containing one or more modules and a __init__.py file, which can be empty or contain some initialization code for the package.\nWe can import certain modules from any of these packages using dot notation. For example to import module_1 from the above package, we could use either of the following code snippets\nimport dairy_processing.calculations.density or\nfrom dairy_processing.calculations import density Note: We can also import specific functions as shown in the python modules section.\nPackaging flow Publishing a package requires a flow from the author’s source code to an end user’s Python environment. The steps to achieve this are:\nHave a source tree containing the package. This is typically a checkout from a version control system (VCS).\nPrepare a configuration file describing the package metadata (name, version and so forth) and how to create the build artifacts. For most packages, this will be a pyproject.toml file, maintained manually in the source tree.\nCreate build artifacts to be sent to the package distribution service (usually PyPI); these will normally be a source distribution (“sdist”) and one or more built distributions (“wheels”). These are made by a build tool using the configuration file from the previous step. Often there is just one generic wheel for a pure Python package.\nUpload the build artifacts to the package distribution service.\nAt that point, the package is present on the package distribution service. To use the package, end users must:\nDownload one of the package’s build artifacts from the package distribution service.\nInstall it in their Python environment, usually in its site-packages directory. This step may involve a build/compile step which, if needed, must be described by the package metadata.\nThese last 2 steps are typically performed by pip when an end user runs pip install.\nPyenv and Poetry to the rescue Poetry is a package which helps manage python packaging and dependency management. Pyenv allows you to manage multiple python versions on your computer. Installation of these tools is out of the scope of this guide.\nFirst we want to ensure our package is using the correct python version, this can be done by running pyenv local 3.10.5 from within the root directory (note: multiple versions can be selected using pyenv local 3.8.13 3.9.13 3.10.5 which would be required to run tox locally). Next we want to init a poetry project, which can be done using poetry init. Don\u0026rsquo;t worry about specifiying dependencies interactively as these can be added easily later, it\u0026rsquo;s important that the name is same as the name of the root package.\nIf you check the pyproject.toml file you should have something similar to below.\n[tool.poetry] name = \u0026#34;rootpackage\u0026#34; version = \u0026#34;0.0.0\u0026#34; description = \u0026#34;test package\u0026#34; authors = [\u0026#34;Your Name (your.name@email.com)\u0026#34;] [tool.poetry.dependencies] python = \u0026#34;^3.10\u0026#34; [tool.poetry.dev-dependencies] [build-system] requires = [\u0026#34;poetry-core\u0026gt;=1.0.0\u0026#34;] build-backend = \u0026#34;poetry.core.masonry.api\u0026#34; You need to ensure that your existing package is in a subfolder ./rootpackage or ./src/rootpackage and that there is a __init__.py file present. If your package needs dependencies, these can be added with poetry add {depa} {depb}, development dependencies can be added using the -D flag poetry add -D pytest.\nYou can install the package and dependencies locally using poetry install, you can build the package using poetry build and you can publish the package using poetry publish.\nLibrary inconsistency.. In theory we can use pyproject.toml to define all our project metadata and configuration for different tooling, however reality is a little more complicated. Some tools fully support pyproject.toml, some have partial support and some don\u0026rsquo;t support it at all. In practice this means you may have some files like tox.ini and setup.cfg to contain certain configurations.\nflake8 Flake8 is one the current python packages which chooses to ignore the pyproject.toml file, meaning that the easiest way to setup the configuration is to add code snippet to one of either setup.cfg, tox.ini or .flake8\n[flake8] max-line-length = 88 extend-ignore = E203 per-file-ignores = __init__.py:F401 pytest pytest is a library which has implemented a bridge to the existing .ini configuration from version 6.0, in future they will fully utilize the rich TOML format and as such have reserved [tool.pytest] for this future use.\n# pyproject.toml [tool.pytest.ini_options] minversion = \u0026#34;6.0\u0026#34; addopts = \u0026#34;-ra -q\u0026#34; testpaths = [ \u0026#34;tests\u0026#34;, \u0026#34;integration\u0026#34;, ] tox Tox has partial support for pyproject.toml by offering to inline existing ini-style fomat under the tool.tox.legacy_tox_ini key as a multi-line string. An example of this is shown below.\n[tool.tox] legacy_tox_ini = \u0026#34;\u0026#34;\u0026#34; [tox] envlist = py27,py36 [testenv] deps = pytest \u0026gt;= 3.0.0, \u0026lt;4 commands = pytest \u0026#34;\u0026#34;\u0026#34; Typically it is easier to avoid this and use the tox.ini, tox are targetting full toml support with their new release.\nSummary Python has moved away from setup.py and setup.cfg to the new pyproject.toml file, most tools support pyproject.toml in some form, however don\u0026rsquo;t expect them all to be perfect, and there are some who just don\u0026rsquo;t support it yet. It is now easier than ever to setup a new package, but there is a lot of conflicting documentation due to this migration.\nPoetry is a powerful tool to assist with managing packaging, and we didn\u0026rsquo;t even scratch the surface here. In a future post I will walk through how this is actually setup and used in a DevOps pipeline along with all the common tooling (pypi, flake8, sphinx, coverage, pytest). There are also alternatives like hatch worth investigation, and also now more than ever, it is worth considering whether these tools are even required.\n","permalink":"http://localhost:1313/posts/python-packaging-2022/","summary":"It\u0026rsquo;s not suprise that new developers are confused when it comes to developing python libraries, there has been major changes over the years when it comes to how to work with python packages, and currently you may ask yourself what to use out of setup.py, setup.cfg and pyproject.toml. Before we look at these, it is important to first make sure we properly understand python modules and packages.\nPython Modules In python a module is simply a file with a .","title":"Python Packages in 2022"}]